<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>probabilistic programming on Maciek's blog</title><link>http://mmakowski.github.io/blog/tags/probabilistic-programming/</link><description>Recent content in probabilistic programming on Maciek's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 07 Aug 2018 00:00:00 +0000</lastBuildDate><atom:link href="http://mmakowski.github.io/blog/tags/probabilistic-programming/index.xml" rel="self" type="application/rss+xml"/><item><title>Precision confidence interval in the presence of unreliable labels</title><link>http://mmakowski.github.io/blog/technology/precision-confidence-interval-in-the-presence-of-unreliable-labels/</link><pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate><guid>http://mmakowski.github.io/blog/technology/precision-confidence-interval-in-the-presence-of-unreliable-labels/</guid><description>Consider a binary classifier that produced the following counts of true positives (TP) and false positives (FP) on test data.
tp_observed = 5285 fp_observed = 3184 We would like to know the 95% confidence interval for precision metric of this classifier. Goutte and Gaussier showed that precision follows the Beta distribution with the counts of TPs and FPs as parameters, adjusted for prior. We will use uniform prior, $Beta(1,1)$.
import pymc3 as pm with pm.</description></item></channel></rss>